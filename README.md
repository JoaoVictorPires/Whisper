## Whisper
Whisper √© um modelo de reconhecimento de fala de uso geral. Ele foi treinado com um grande conjunto de dados de √°udio diversificado e √© um modelo multitarefa capaz de realizar reconhecimento de fala multil√≠ngue, tradu√ß√£o de fala e identifica√ß√£o de idioma.

## M√©todo

![Approach](https://raw.githubusercontent.com/openai/whisper/main/approach.png)

O modelo utiliza um Transformer de sequ√™ncia para sequ√™ncia treinado em v√°rias tarefas de processamento de fala, incluindo reconhecimento de fala multil√≠ngue, tradu√ß√£o de fala, identifica√ß√£o de idioma falado e detec√ß√£o de atividade de voz. Essas tarefas s√£o representadas como uma sequ√™ncia de tokens prevista pelo decodificador, permitindo que um √∫nico modelo substitua v√°rias etapas de um pipeline tradicional de processamento de fala. O formato de treinamento multitarefa usa um conjunto de tokens especiais que funcionam como especificadores de tarefa ou alvos de classifica√ß√£o.

## Como utilizar!

O treinamento e os testes foram realizados usando Python 3.9.9 e PyTorch 1.10.1. No entanto, o c√≥digo √© compat√≠vel com Python 3.8‚Äì3.11 e vers√µes recentes do PyTorch. O projeto tamb√©m depende de alguns pacotes Python, como tiktoken da OpenAI para tokeniza√ß√£o r√°pida. Para instalar a √∫ltima vers√£o do Whisper:
        
        pip install -U openai-whisper

Para instalar diretamente do reposit√≥rio GitHub com todas as deped√™ncias:

        pip install git+https://github.com/openai/whisper.git

Para atualizar o pacote execute:

        pip install --upgrade --no-deps --force-reinstall git+https://github.com/openai/whisper.git

√â necess√°rio o utilit√°rio de linha de comando [`ffmpeg`](https://ffmpeg.org/), que pode ser instalado pelos gerenciadores de pacotes:

```bash

# Ubuntu ou Debian
sudo apt update && sudo apt install ffmpeg

# Arch Linux
sudo pacman -S ffmpeg

# macOS (Homebrew)
brew install ffmpeg

# Windows (Chocolatey)
choco install ffmpeg

# Windows (Scoop)
scoop install ffmpeg

```

Tamb√©m pode ser necess√°rio instalar [`rust`](http://rust-lang.org). Caso o [tiktoken](https://github.com/openai/tiktoken)n√£o fornec√ßa um pre-built, siga [Getting started page](https://www.rust-lang.org/learn/get-started) de introdu√ß√£o ao Rust para configurar o ambiente. Adicionalmente, configure a vari√°vel de ambiente `PATH`
`export PATH="$HOME/.cargo/bin:$PATH"`. Se ocorrer o erro `No module named 'setuptools_rust'` `setuptools_rust'`, instale o m√≥dulo com:

```bash
pip install setuptools-rust
```

## Modelos dispon√≠veis e idiomas

O Whisper oferece seis tamanhos de modelo, com vers√µes somente em ingl√™s ou multil√≠ngues, balanceando velocidade e precis√£o.

|  Size  | Parameters | English-only model | Multilingual model | Required VRAM | Relative speed |
|:------:|:----------:|:------------------:|:------------------:|:-------------:|:--------------:|
|  tiny  |    39 M    |     `tiny.en`      |       `tiny`       |     ~1 GB     |      ~10x      |
|  base  |    74 M    |     `base.en`      |       `base`       |     ~1 GB     |      ~7x       |
| small  |   244 M    |     `small.en`     |      `small`       |     ~2 GB     |      ~4x       |
| medium |   769 M    |    `medium.en`     |      `medium`      |     ~5 GB     |      ~2x       |
| large  |   1550 M   |        N/A         |      `large`       |    ~10 GB     |       1x       |
| turbo  |   809 M    |        N/A         |      `turbo`       |     ~6 GB     |      ~8x       |


Os modelos `tiny.en` e `base.en`, voltados apenas para ingl√™s, apresentam melhor desempenho. Para tarefas multil√≠ngues, o modelo turbo oferece maior velocidade com uma leve redu√ß√£o na precis√£o. Abaixo, uma compara√ß√£o de taxas de erro (WER e CER) por idioma para os modelos `large-v3` e `large-v2`:

![WER breakdown by language](https://github.com/openai/whisper/assets/266841/f4619d66-1058-4005-8f67-a9d811b77c62)

## Uso via linha de comando

O comando a seguir ir√° transcrever a fala em arquivos de √°udio, utilizando o modelo `turbo`:

     whisper audio.flac audio.mp3 audio.wav --model turbo

A configura√ß√£o padr√£o (que seleciona o modelo `turbo`) funciona bem para transcri√ß√£o em ingl√™s. Para transcrever um arquivo de √°udio contendo fala em um idioma diferente do ingl√™s, voc√™ pode especificar o idioma usando a op√ß√£o `--language`:

     whisper japanese.wav --language Japanese

Adicionando `--task translate`, a fala ser√° traduzida para o ingl√™s:

    whisper japanese.wav --language Japanese --task translate

Execute o seguinte comando para visualizar todas as op√ß√µes dispon√≠veis:

    whisper --help

Consulte [tokenizer.py](https://github.com/openai/whisper/blob/main/whisper/tokenizer.py) para a lista de todos os idiomas dispon√≠veis.

## Uso com Python

A transcri√ß√£o tamb√©m pode ser realizada diretamente no Python:

```python
import whisper

model = whisper.load_model("turbo")
result = model.transcribe("audio.mp3")
print(result["text"])
```

Internamente, o m√©todo `transcribe()` l√™ o arquivo inteiro e processa o √°udio em janelas deslizantes de 30 segundos, realizando previs√µes sequenciais autoregressivas em cada janela.

Abaixo est√° um exemplo de uso de `whisper.detect_language()` e `whisper.decode()`, que fornecem acesso de n√≠vel mais baixo ao modelo:

```python
import whisper

model = whisper.load_model("turbo")

# Carregar o √°udio e ajust√°-lo para caber em 30 segundos
audio = whisper.load_audio("audio.mp3")
audio = whisper.pad_or_trim(audio)

# Criar um espectrograma log-Mel e mov√™-lo para o mesmo dispositivo do modelo
mel = whisper.log_mel_spectrogram(audio, n_mels=model.dims.n_mels).to(model.device)

# Detectar o idioma falado
_, probs = model.detect_language(mel)
print(f"Detected language: {max(probs, key=probs.get)}")

# Decodificar o √°udio
options = whisper.DecodingOptions()
result = whisper.decode(model, mel, options)

# Imprimir o texto reconhecido
print(result.text)
```
## Transformer
![Imagem do WhatsApp de 2024-12-02 √†(s) 17 26 47_d4b0dc98](https://github.com/user-attachments/assets/7c55cc3d-5cb9-466b-b7d4-5fa45d948972)

## Seq2Seq
![Imagem do WhatsApp de 2024-12-02 √†(s) 17 26 22_5b56a549](https://github.com/user-attachments/assets/d7aa6c22-eef8-4f58-abf0-7bfe303a71c9)
## Mais exemplos


Por favor, utilize a [üôå Show and tell](https://github.com/openai/whisper/discussions/categories/show-and-tell) em Discuss√µes para compartilhar mais exemplos de uso do Whisper e extens√µes de terceiros, como demonstra√ß√µes web, integra√ß√µes com outras ferramentas, ports para diferentes plataformas, etc.
